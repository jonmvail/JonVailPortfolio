{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pronouncing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jmvail\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\jmvail\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\jmvail\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\jmvail\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\jmvail\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./Data/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metered "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-bb3eab49bc2b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpronouncing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstresses_for_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpronouncing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mphones_for_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sasdsy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\dsi\\lib\\site-packages\\pronouncing\\__init__.py\u001b[0m in \u001b[0;36mstresses_for_word\u001b[1;34m(find)\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mreturns\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mpossible\u001b[0m \u001b[0mstress\u001b[0m \u001b[0mpatterns\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m     \"\"\"\n\u001b[1;32m--> 129\u001b[1;33m     \u001b[0mphone_lists\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mphones_for_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstresses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphone_lists\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dsi\\lib\\site-packages\\pronouncing\\__init__.py\u001b[0m in \u001b[0;36mphones_for_word\u001b[1;34m(find)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \"\"\"\n\u001b[0;32m     96\u001b[0m     \u001b[0minit_cmu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mlookup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "pronouncing.stresses_for_word(pronouncing.phones_for_word(\"sasdsy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1122100101111201000101110111111000110111110110011011100102101'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"this process however afforded me no means of ascertaining the dimensions of my dungeon as i might make its circuit and return to the point whence i set out without being aware of the fact so perfectly uniform seemed the wall\"\n",
    "phones = [pronouncing.stresses_for_word(p)[0] for p in text.split()]\n",
    "''.join(map(str, phones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HH ER1 M AH0 N',\n",
       " 'IH1 Z',\n",
       " 'DH AH0',\n",
       " 'K R UW1 L AH0 S T',\n",
       " 'M AH1 N TH',\n",
       " 'B R IY1 D IH0 NG',\n",
       " 'L AY1 L AE2 K S',\n",
       " 'AW1 T',\n",
       " 'AH1 V',\n",
       " 'DH AH0',\n",
       " 'D EH1 D']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-acef8a76047d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"the astronomer perhaps at this point took refuge in the suggestion of non luminosity and here analogy was suddenly let fall\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mphones\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpronouncing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstresses_for_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphones\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-70-acef8a76047d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"the astronomer perhaps at this point took refuge in the suggestion of non luminosity and here analogy was suddenly let fall\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mphones\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpronouncing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstresses_for_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphones\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "text = \"the astronomer perhaps at this point took refuge in the suggestion of non luminosity and here analogy was suddenly let fall\"\n",
    "phones = [pronouncing.stresses_for_word(p)[0] for p in text.split()]\n",
    "''.join(map(str, phones))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need a \"Try\" and \"Except\" for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HH ER1 M AH0 N',\n",
       " 'IH1 Z',\n",
       " 'DH AH0',\n",
       " 'K R UW1 L AH0 S T',\n",
       " 'M AH1 N TH',\n",
       " 'B R IY1 D IH0 NG',\n",
       " 'L AY1 L AE2 K S',\n",
       " 'AW1 T',\n",
       " 'AH1 V',\n",
       " 'DH AH0',\n",
       " 'D EH1 D']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pronouncing.stresses(s[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syllable_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"herman is the cruelest month breeding lilacs out of the dead\"\n",
    "phones = [pronouncing.phones_for_word(p)[0] for p in text.split()]\n",
    "max([pronouncing.syllable_count(p) for p in phones])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HH ER1 M AH0 N',\n",
       " 'IH1 Z',\n",
       " 'DH AH0',\n",
       " 'K R UW1 L AH0 S T',\n",
       " 'M AH1 N TH',\n",
       " 'B R IY1 D IH0 NG',\n",
       " 'L AY1 L AE2 K S',\n",
       " 'AW1 T',\n",
       " 'AH1 V',\n",
       " 'DH AH0',\n",
       " 'D EH1 D']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# break down scentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'youth',\n",
       " 'passed',\n",
       " 'in',\n",
       " 'solitude',\n",
       " ',',\n",
       " 'my',\n",
       " 'best',\n",
       " 'years',\n",
       " 'spent',\n",
       " 'under',\n",
       " 'your',\n",
       " 'gentle',\n",
       " 'and',\n",
       " 'feminine',\n",
       " 'fosterage',\n",
       " ',',\n",
       " 'has',\n",
       " 'so',\n",
       " 'refined',\n",
       " 'the',\n",
       " 'groundwork',\n",
       " 'of',\n",
       " 'my',\n",
       " 'character',\n",
       " 'that',\n",
       " 'I',\n",
       " 'can',\n",
       " 'not',\n",
       " 'overcome',\n",
       " 'an',\n",
       " 'intense',\n",
       " 'distaste',\n",
       " 'to',\n",
       " 'the',\n",
       " 'usual',\n",
       " 'brutality',\n",
       " 'exercised',\n",
       " 'on',\n",
       " 'board',\n",
       " 'ship',\n",
       " ':',\n",
       " 'I',\n",
       " 'have',\n",
       " 'never',\n",
       " 'believed',\n",
       " 'it',\n",
       " 'to',\n",
       " 'be',\n",
       " 'necessary',\n",
       " ',',\n",
       " 'and',\n",
       " 'when',\n",
       " 'I',\n",
       " 'heard',\n",
       " 'of',\n",
       " 'a',\n",
       " 'mariner',\n",
       " 'equally',\n",
       " 'noted',\n",
       " 'for',\n",
       " 'his',\n",
       " 'kindliness',\n",
       " 'of',\n",
       " 'heart',\n",
       " 'and',\n",
       " 'the',\n",
       " 'respect',\n",
       " 'and',\n",
       " 'obedience',\n",
       " 'paid',\n",
       " 'to',\n",
       " 'him',\n",
       " 'by',\n",
       " 'his',\n",
       " 'crew',\n",
       " ',',\n",
       " 'I',\n",
       " 'felt',\n",
       " 'myself',\n",
       " 'peculiarly',\n",
       " 'fortunate',\n",
       " 'in',\n",
       " 'being',\n",
       " 'able',\n",
       " 'to',\n",
       " 'secure',\n",
       " 'his',\n",
       " 'services',\n",
       " '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(train.text[5])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'DT'),\n",
       " ('youth', 'NN'),\n",
       " ('passed', 'VBN'),\n",
       " ('in', 'IN'),\n",
       " ('solitude', 'NN'),\n",
       " (',', ','),\n",
       " ('my', 'PRP$'),\n",
       " ('best', 'JJS'),\n",
       " ('years', 'NNS'),\n",
       " ('spent', 'VBD'),\n",
       " ('under', 'IN'),\n",
       " ('your', 'PRP$'),\n",
       " ('gentle', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('feminine', 'JJ'),\n",
       " ('fosterage', 'NN'),\n",
       " (',', ','),\n",
       " ('has', 'VBZ'),\n",
       " ('so', 'RB'),\n",
       " ('refined', 'VBN'),\n",
       " ('the', 'DT'),\n",
       " ('groundwork', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('my', 'PRP$'),\n",
       " ('character', 'NN'),\n",
       " ('that', 'IN'),\n",
       " ('I', 'PRP'),\n",
       " ('can', 'MD'),\n",
       " ('not', 'RB'),\n",
       " ('overcome', 'VB'),\n",
       " ('an', 'DT'),\n",
       " ('intense', 'JJ'),\n",
       " ('distaste', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('usual', 'JJ'),\n",
       " ('brutality', 'NN'),\n",
       " ('exercised', 'VBN'),\n",
       " ('on', 'IN'),\n",
       " ('board', 'NN'),\n",
       " ('ship', 'NN'),\n",
       " (':', ':'),\n",
       " ('I', 'PRP'),\n",
       " ('have', 'VBP'),\n",
       " ('never', 'RB'),\n",
       " ('believed', 'VBN'),\n",
       " ('it', 'PRP'),\n",
       " ('to', 'TO'),\n",
       " ('be', 'VB'),\n",
       " ('necessary', 'JJ'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('when', 'WRB'),\n",
       " ('I', 'PRP'),\n",
       " ('heard', 'VBD'),\n",
       " ('of', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('mariner', 'NN'),\n",
       " ('equally', 'RB'),\n",
       " ('noted', 'VBD'),\n",
       " ('for', 'IN'),\n",
       " ('his', 'PRP$'),\n",
       " ('kindliness', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('heart', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('the', 'DT'),\n",
       " ('respect', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('obedience', 'NN'),\n",
       " ('paid', 'VBN'),\n",
       " ('to', 'TO'),\n",
       " ('him', 'PRP'),\n",
       " ('by', 'IN'),\n",
       " ('his', 'PRP$'),\n",
       " ('crew', 'NN'),\n",
       " (',', ','),\n",
       " ('I', 'PRP'),\n",
       " ('felt', 'VBD'),\n",
       " ('myself', 'PRP'),\n",
       " ('peculiarly', 'RB'),\n",
       " ('fortunate', 'VBP'),\n",
       " ('in', 'IN'),\n",
       " ('being', 'VBG'),\n",
       " ('able', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('secure', 'VB'),\n",
       " ('his', 'PRP$'),\n",
       " ('services', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(tokens)\n",
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DT'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3oAAABiCAIAAACJXDZEAAAJMmlDQ1BkZWZhdWx0X3JnYi5pY2MAAEiJlZVnUJNZF8fv8zzphUASQodQQ5EqJYCUEFoo0quoQOidUEVsiLgCK4qINEWQRQEXXJUia0UUC4uCAhZ0gywCyrpxFVFBWXDfGZ33HT+8/5l7z2/+c+bec8/5cAEgiINlwct7YlK6wNvJjhkYFMwE3yiMn5bC8fR0A9/VuxEArcR7ut/P+a4IEZFp/OW4uLxy+SmCdACg7GXWzEpPWeGjy0wPj//CZ1dYsFzgMt9Y4eh/eexLzr8s+pLj681dfhUKABwp+hsO/4b/c++KVDiC9NioyGymT3JUelaYIJKZttIJHpfL9BQkR8UmRH5T8P+V/B2lR2anr0RucsomQWx0TDrzfw41MjA0BF9n8cbrS48hRv9/z2dFX73kegDYcwAg+7564ZUAdO4CQPrRV09tua+UfAA67vAzBJn/eqiVDQ0IgALoQAYoAlWgCXSBETADlsAWOAAX4AF8QRDYAPggBiQCAcgCuWAHKABFYB84CKpALWgATaAVnAad4Dy4Aq6D2+AuGAaPgRBMgpdABN6BBQiCsBAZokEykBKkDulARhAbsoYcIDfIGwqCQqFoKAnKgHKhnVARVApVQXVQE/QLdA66At2EBqGH0Dg0A/0NfYQRmATTYQVYA9aH2TAHdoV94fVwNJwK58D58F64Aq6HT8Id8BX4NjwMC+GX8BwCECLCQJQRXYSNcBEPJBiJQgTIVqQQKUfqkVakG+lD7iFCZBb5gMKgaCgmShdliXJG+aH4qFTUVlQxqgp1AtWB6kXdQ42jRKjPaDJaHq2DtkDz0IHoaHQWugBdjm5Et6OvoYfRk+h3GAyGgWFhzDDOmCBMHGYzphhzGNOGuYwZxExg5rBYrAxWB2uF9cCGYdOxBdhK7EnsJewQdhL7HkfEKeGMcI64YFwSLg9XjmvGXcQN4aZwC3hxvDreAu+Bj8BvwpfgG/Dd+Dv4SfwCQYLAIlgRfAlxhB2ECkIr4RphjPCGSCSqEM2JXsRY4nZiBfEU8QZxnPiBRCVpk7ikEFIGaS/pOOky6SHpDZlM1iDbkoPJ6eS95CbyVfJT8nsxmpieGE8sQmybWLVYh9iQ2CsKnqJO4VA2UHIo5ZQzlDuUWXG8uIY4VzxMfKt4tfg58VHxOQmahKGEh0SiRLFEs8RNiWkqlqpBdaBGUPOpx6hXqRM0hKZK49L4tJ20Bto12iQdQ2fRefQ4ehH9Z/oAXSRJlTSW9JfMlqyWvCApZCAMDQaPkcAoYZxmjDA+SilIcaQipfZItUoNSc1Ly0nbSkdKF0q3SQ9Lf5RhyjjIxMvsl+mUeSKLktWW9ZLNkj0ie012Vo4uZynHlyuUOy33SB6W15b3lt8sf0y+X35OQVHBSSFFoVLhqsKsIkPRVjFOsUzxouKMEk3JWilWqUzpktILpiSTw0xgVjB7mSJleWVn5QzlOuUB5QUVloqfSp5Km8oTVYIqWzVKtUy1R1WkpqTmrpar1qL2SB2vzlaPUT+k3qc+r8HSCNDYrdGpMc2SZvFYOawW1pgmWdNGM1WzXvO+FkaLrRWvdVjrrjasbaIdo12tfUcH1jHVidU5rDO4Cr3KfFXSqvpVo7okXY5upm6L7rgeQ89NL0+vU++Vvpp+sP5+/T79zwYmBgkGDQaPDamGLoZ5ht2GfxtpG/GNqo3uryavdly9bXXX6tfGOsaRxkeMH5jQTNxNdpv0mHwyNTMVmLaazpipmYWa1ZiNsulsT3Yx+4Y52tzOfJv5efMPFqYW6RanLf6y1LWMt2y2nF7DWhO5pmHNhJWKVZhVnZXQmmkdan3UWmijbBNmU2/zzFbVNsK20XaKo8WJ45zkvLIzsBPYtdvNcy24W7iX7RF7J/tC+wEHqoOfQ5XDU0cVx2jHFkeRk4nTZqfLzmhnV+f9zqM8BR6f18QTuZi5bHHpdSW5+rhWuT5z03YTuHW7w+4u7gfcx9aqr01a2+kBPHgeBzyeeLI8Uz1/9cJ4eXpVez33NvTO9e7zofls9Gn2eedr51vi+9hP0y/Dr8ef4h/i3+Q/H2AfUBogDNQP3BJ4O0g2KDaoKxgb7B/cGDy3zmHdwXWTISYhBSEj61nrs9ff3CC7IWHDhY2UjWEbz4SiQwNCm0MXwzzC6sPmwnnhNeEiPpd/iP8ywjaiLGIm0iqyNHIqyiqqNGo62ir6QPRMjE1MecxsLDe2KvZ1nHNcbdx8vEf88filhICEtkRcYmjiuSRqUnxSb7JicnbyYIpOSkGKMNUi9WCqSOAqaEyD0tandaXTlz/F/gzNjF0Z45nWmdWZ77P8s85kS2QnZfdv0t60Z9NUjmPOT5tRm/mbe3KVc3fkjm/hbKnbCm0N39qzTXVb/rbJ7U7bT+wg7Ijf8VueQV5p3tudATu78xXyt+dP7HLa1VIgViAoGN1tubv2B9QPsT8M7Fm9p3LP58KIwltFBkXlRYvF/OJbPxr+WPHj0t6ovQMlpiVH9mH2Je0b2W+z/0SpRGlO6cQB9wMdZcyywrK3BzcevFluXF57iHAo45Cwwq2iq1Ktcl/lYlVM1XC1XXVbjXzNnpr5wxGHh47YHmmtVagtqv14NPbogzqnuo56jfryY5hjmceeN/g39P3E/qmpUbaxqPHT8aTjwhPeJ3qbzJqamuWbS1rgloyWmZMhJ+/+bP9zV6tua10bo63oFDiVcerFL6G/jJx2Pd1zhn2m9az62Zp2WnthB9SxqUPUGdMp7ArqGjzncq6n27K7/Ve9X4+fVz5ffUHyQslFwsX8i0uXci7NXU65PHsl+spEz8aex1cDr97v9eoduOZ67cZ1x+tX+zh9l25Y3Th/0+LmuVvsW523TW939Jv0t/9m8lv7gOlAxx2zO113ze92D64ZvDhkM3Tlnv296/d5928Prx0eHPEbeTAaMip8EPFg+mHCw9ePMh8tPN4+hh4rfCL+pPyp/NP637V+bxOaCi+M24/3P/N59niCP/Hyj7Q/Fifzn5Ofl08pTTVNG02fn3Gcufti3YvJlykvF2YL/pT4s+aV5quzf9n+1S8KFE2+Frxe+rv4jcyb42+N3/bMec49fZf4bmG+8L3M+xMf2B/6PgZ8nFrIWsQuVnzS+tT92fXz2FLi0tI/QiyQvpTNDAsAAAAJcEhZcwAADdcAAA3XAUIom3gAAAAddEVYdFNvZnR3YXJlAEdQTCBHaG9zdHNjcmlwdCA5LjIzKPqaOAAAHg1JREFUeJzt3U9sG9edB/BHiqI0lCxp5MiyhbayqSRbWOgCDZXsxUATiCy2LdxeQh2N5mAJPTWXijz6VJDoyS1QgLwUsG/k3gLkwgmQAgoWG3OyBRY0us1yZBeFZFm1RrKt/3+4h1/08jz/NEPOiJT8/RwSajycefPmzXu/ee/NMNRoNBgAAAAAQDDC7U4AAAAAAJxnCDcBAAAAIEAINwEAAAAgQAg3AQAAACBACDcBAAAAIEAINwEAAAAgQJF2JwAA4HVXLper1erMzIwsy/F4vN3JAQDwGXo3AQDaKZPJ6LqezWYVRSkUCu1ODgCA/0J4zTsAQBul0+lyuUyfFUVJJpPtTQ8AgO8QbgIAtJOqqoVCQZblqampdDrd7uQAAPgP4SYAQEegGZz5fL7dCQEA8BnmbgIAtFMmk6EP6XRa1/X2JgYAIAh4Mh0AoJ0URaGIU9f1VCrV7uQAAPgPg+kAAG2m67qqqnhICADOK4SbAAAAABAgzN0EAAAAgAAh3AQAAACAACHcBAAAAIAA4cl0AIC20Z4+LXz++X9p2rOXL/cPD/8tHv/4xz/+4fh4u9MFAOAnPCoEAOCZ+uiRvrlJn796/Jgxtry+vrS+vndwoK2u7h4cMMb2Dg9X1tePGDs8OjpqNI6OjpqrbUOMsVAoHAqFQ6Hurq7e7u6RCxcYYz2RyOjg4PjFi2+NjjLGrgwOXhkaYowlrl6V+/p8OUwAAF8g3ASA14hSqxmWfPKXvzzZ2BCXPFxaerGzQ5+39/YOj470ra2jIKvKvmj04OjoqNHYPzz0fePhUKgvGpV6ehhj3eFwpKtrdGDg6htv8BXeHB394Pvf53/KfX2Jq1d9TwYAvM4QbgJAp9M3N9VHj15ZsrVVXVykzzv7+xQvfr2yQkvWjvsdHz97dmqJDDFmrkxj0ei/XL7MGHv32rXe7u5wKPS3lZUvvv56Y3ubMfbh1FRqcjL97ru8M1Kp1XKffvrZw4dDsdiPJyf//Qc/+Ovy8n/W68vr6//39OmpHYvB6MBAb3c3fR6QJDkWuzw4SH9S2JqanBTXj4+MxC9dOuVEAkAnQ7gJAEHRnj7VVldfWbK6Wn81bNK3tv73yZMX29t8yfLGhqG7MWiWkSLX29090NvLGNs9OBiKxZ5sbNBYuWj6+nXGWGJ8nDE2celSfGSEh1z65mb5wYNKrfYf1SpjbCgWS7/77tTVq2KUaSAGnbM/+lH2Zz+jNWkEv1Kr6Vtb2urqZw8fit8av3jx4Oho/+Bg7/BwfWvLvNnurq4gek/dGL94UYpGpeOY9a3R0cuDgzyEHe7vT7w6XTX5avwKAGcdwk0AMDKPOKuPH6+9fCku0VZX9eOYZv/w8MX29t9WVjZ3d4NIT1c4HOvuZoxt7e0duqiy3ujvp9W29/Z29vftVhuKxWjUeO3ly7dGR1eeP9/e2wuHw7v7+5Yh7zvj43JfX3xkRI7FeIRkFxhZRpnUl+nyqO2CThEF9HR21MeP1UePxEDzzUuXLvT2DkhSb3f3oCQ929xkjBmCVDIoSeFwuDsclqJRhy7hSDjcG40yxl4eTzZw4BzEe3J5cPDKcX+qFI1K0aghPJ26dk2OxfifmA8A0GkQbgKcE+YRZ8ZYxSpwNCyxjD/81ReN9vX0MMb2Dw9ZKERhzfrW1obQqWnnzUuXeJ9cOBSiOZTO3702MhIfGaHPPC4Z7usLhULL6+tbe3tDsZj6+LG+ufmVKTcoBpVjMdoCDRO7f/hGe/q0XK2WvvySttxElGngJugUUTGgXmT18WNtdXVR6GCmnEmMjzcYe6O/f+TChb8uL7PjmwfDytzEpUsHh4ejAwO7Bwebu7sXenude6Bp8P35zg71Cu/s7688f37ikfb39tKH3f1933th3xodlWOxC5JEf9I9g7gC5gMABArhJkCbuRlxZqYw0TJUcuNCb2+kq+vF9vbB0ZH7b41cuPAdWWaMPd/ZOTg8pN6y3YMD3pcWjUT41EkH10ZGvjs8TEPn/b29u8ddjxvb27FolJ00ks7jSB4OMiFQSFy9qm9uUuREMzsp0wx9fkQc/qa+sVYiDEOUeW1kJD01NfPee371sXkNOg0oAK0uLmqrq9rqqlhyKLym8Cs1OUmZQN3bvEubstHytuR7Fy9e7Ovr7+2VY7EnGxtXhoZ6IpF/6LoUjdp9hfT19FwbGaGSMDowwBhbef58uK+PMfZyd/fEstQTidD80ec7O93hb14gbTeLoHW8I5wzdK/SDApxCeYDAIgQbgI0SXwVzjdLHEec+RLLDqQTGcILr/EiDQQzxrb39rb39q4MDfX39KxvbVHzLEWjowMDYhv/33//+4nbNLTBk2NjPFLcPTjoiUQYY082NkKhUKSry/nAT4wjDYdP8RD13Tr0zInD3xQQ+DvMqj56VPryS+Xhw4CiTIMWg04R3eTYTQN9Z3ycpp9OXbsWHxkRD4e+yAN6525RHtM/2djojkTeHh1dXl+PRiKMMfoWO+nG6V+/852ucJgxdmVo6ODwcH1rS+7rG5Skl7u7/3zxgnor3VxTfT093x0e3t7bY4wNxmK9kcjG9vbO/j7NKD04Ovqff/zDZdZFwuELkmS49k8k9rizV8s5wXwAON8QbsJrxzwxMbgR50FJGjpuQp7v7DDGaByZMWb5xIkDHi8SCqH4Q9kXJOnK4ODfnjzhKzwTmkPL7j0zCg6I2HlzeXCQD27Wnz6lI+KRtL9xpIgCeurrpejEMjShXRiGvwPtW6Ios1yt0oG/Mz6evH49uCjTwMegU0Sj8HbTQPkoPEXtdtnrvlvUcNamrl2LhMMvd3bo1aH8kuRXokMZFm97JkZGaLXE1av0JlR6JolHt86b4sTZosP9/fxRNrmvryscXl5fZ4wNSFKkq8vlxcW/Th+cZxVbEi9PZpoPYH7cCq9fhY6CcBPOBvOIs/gqHM6vEec3L12KdHVRXwgZHRgwzD/z+pIdy3iR/znc3//26KjYv/hM6CgVe0ld9o+KuxP3JTZLXeHw4XEXKW/gTyGOFBl6y+wCFB5S+DX83QRzlDnz3nvpqam2TPILKOg074VPAzVcTXRGEuPjVKJOPOPNdYsyocRSjMtHFficE5cdpbzE8sthuL//Qk/Py93d/t7et0dHxdEJ8YpzE1Dyy40eO+PLeTc/Y+zy4GBtaenbDPEy0DE6MLAnzGcd6O31er+K+QDQXgg3IVjmEWfLV+GYJy82N+L8w+99j3/e3t/f3tujaWGE4sUBSYqEw8zdeLHIa++CSGzJxON1GRCLI3GGYTjxEQfePIjRebviSBF/jEkc/rZsxanZFt8o1PZRRXrAvPzgAaW2vVGmwekEnSJxGqi+tWW4MZi+fp1PA/V04lrpFjXfeIiPzXnqKGXCZc4vZz7Mnbh6lbrYDVtmr97ouhkJEe8GaUdPNjZ29vd5jqmPHomv4he37+kumg+w7B8ebu/vDxw/j+Xy+S27NDOr+QB43AocINwEa25GnM0TEz2NK3H8tvv59vbB4SFjrL+398rgIJ9ZyKwmF7KW40Xnm3tDYynGzYaI2WtLY+hmEJMh1teWgR2PI8XOXZftaBBxpIEYNLgc/j7xjULtYogyza9k7xynH3SKDNNADYVQnAYqx2JNnGW/ukXttsxevaLpamqio5T2Zag3xIrUMjx1GTvaTXRJTU4ur68vb2xQBy0zzSA31NLuJwXFolH6rVQyGIttCNsZkKTVFy+8vh/XuQbGfIDzDeHmueL84yucXxMTDZPfGWM9kUh/Tw//c31rKxQKDR6/fIQx9vXKCr02r7n9thIvisTOP0MWiZnT4pRHce7/iTf6HR5HmpPKW2iXw99e3yjUFvSyzOqjR2ciyjRob9ApMkwDtXsZE12wLRYJf7tFHXbBrAYKmu4otdyvWIHb1UtuwlPxhlasE8R4jh6hE9sLw120YdDJU1cCjTKJb7Gg5SvPn4tv+/faX2BucZpuAuD0IdzsCC5/fMWvEWdD0MaEi/bRP/9JH96+fPlCb+/XKyvPj+fIXx4c3N7fF9PQxMxIv+JFkSHIFrsQvE7AYvZTHsW0earFWokjeWLMnSgs4NDNMPzt8o1Chh/UOSua+OGfjtU5QacBnwZq9zImPg3Ur/ITXLeo3b5YCx2lPC4Ur3HnBIhVn7hfsbFw00yI4aldpWc4KWKvrSEyNjRVntop+mEC/md3Vxf/LEWjjUbjxfHvC0jR6M7+vtcGyHkCPcN8gCAh3GyJ1x9f+Wad1kacOfPUGcbYxf5+8c+vV1aGhavLUBF4jRe9TjZv7uZSeXXIyXLKo8sqzG7Ko2HUprlh3ODiyNMcVhY7h1y+Uahjh7+9av2HfzpWxwadIj4N1PJlTHwaqPllTL44hW5Ru50y7zOqvXaUWhL7NezCUzfNk1iv2oWndjfDhtn8hjlaTU9RpQqKT8eip7X4dCw+C/brlZXuri7+tn+vbfGJLSB+jtXB6xhunvKPr5j7/813VGIZpQvMbiJO0PGi4d1vrd/buZny6PIo7EaImIspj56cjzhSJL5RiDEW0A/qnBW+//BPxzoTQaeITwNljJlfxsSngTq/jMmvZJxOt6jd3plN/ePQDLXSUWpJrL19eWyf2U83cvlmg2/T9mrLaGiv3TfWYgO9f3jY39MjzgeLdHUdCC8EGIzFDGOMXqOCE+cDnOPXr56lcPOUf3zF0OvOTMWCmUoGY+xiX98zx1d/n7l4UeRyyqPLK9CXKY+enL84UmT5RqFT+EGdsyLoH/7pWGcu6BS5nAbq8mVMvmhLt6hdMsSUnGZHqSUxPPX3sX1i+RaOExk6mAxhg2H40X1/p6EtphPN+1PHhoauDA292NmhdyHTy1/Nu2siPvE6H6Bz+gtOI9w85R9fcZiYyKVMJZWXXXPfp6Hj05BUr+lsb7woCm7Ko3hETU959ISXsXMZRxp0yA/qnBWn/MM/HetMB50GSq1GV7rDNFDGGP9NzlNLWHu7Re3Sw7x3lPJ2yvLpQ78SeQqP7dOHpqs+91NUPY3Liwk2jNQ929x8W3hv6/7hIX984psdvZohvs8HCO71q02Gm5lSiT4EN+JsnphonhXh9b1uYgvNl3uNF0/8LbLOmWtc/Pzz+nH1R0t8fEM4a0coRkfkprpkZzCOtKTUaqUHDzrnB3XOkPynnxY+/7wtP/zTscSgM59Oz77/frtT5A+64bT7Tc7p69flWGzu/ffbe2k01y2an5k5nYSJaeOxlHOTwetYqlcDTWpwj+1PXLrk+4XgfoqqpwjE0LXJb6vc95eZnzlucT6A2Po4X19Nhpuhjz6iD25GnM3BclsisPKDBzN//CPzHi92Tl+0V8nf/e6zhw9dviH8TBwmHdH5iCNdypRKxT//ue0/qHMW5T/9tPTll53zSvbOQUFn9qc/PWcXi4g69sTf5Cz+8pcdOz3XoVu08ac/tTt1jAlxjF1H6Tvj4+qdO21KnQX3j+1PX7+u/OY3bUmkmfspqjy2nv/JT3wM9A2BsvldinYddicm4yzN3QQAAACAMyfc7gQAAAAAwHmGcBMAAAAAAoRwEwAAAAACFKH/KYrCGJNlOZFI0BJN02RZlmX5xE2oqqrrOn2Ox+PxeJyWJJNJvmVa7leiFUXJ5XK0ZYd1CoVCuVw2LHd/XOVyuVqtplIpyhZN0zRNo39KJBJuttAcOrpyueywC0VRKpUKYyyfz3vauKZpuVxubm6On2g7lidR1/VAz6wnp1nwdF1XVVVckkwm7cqYL4rFYr1e93p+DdyfRMYYL95tPKctclMzuNT55Z+I9RJPjLiQMUY1nt35pYpuZmZGlmW+nNd+dLBBV+meiJeGwxl3voLEo6Cj29zc7Dt+VpI3hUFX+4ba3mXzdOJJF5ty93Rdz+Vy9CGfz7s/WJftkTnPDSWzuWQHLaDc9pGbxsIywad8FGG+p0qlksvl+L4LhYKhfXWWyWTEP1OpFG+GS6WSv1dpIpE4sRlOJpM8FhG5PK5MJqPrejab1TSNKlkmHGMmk/GUOV599tlnzjmWTCbz+XwTaYjH47IsW+aMmeVJDPTMNuF0Cp6qqpTbtLtSqcTsy5gvZmdnfdmO+5PIczK4GDpobmoG985E+Wc2J068Luiz3WpU0dG9k2GhqqrFYpEWds6Bi5eGwxl3voI0TSsdv86PMUbHzrNIURRzTgZU7Yu1vftm1/mki+l3r1gsplKpfD5fLBY9nV+X7ZFlnrOWk30KgshtH7lpLOiEUkPGT67lwuB807uZSCQKhUI+ny+Xy5lMRlEUuvlwc8tCEbEsyzwsSyQS09PTlUolmUwmk8lKpdL0kSiKwqu2bDYryzK/kRIjcboX13U9lUpVKhXee0cFgt+ruTwuVVWHh4fpFPITSYEaHWMymcxkMi3eCqiqyq89OjT6nEgk3nnnHb6apmn8snRIs90G7b5eLpdLpdLc3Bw/awaWJ9HHM2uXZk3TMpnM1NTU2tqa8012oAXPvC/q26Ddif065jJGVRLl/MTEBBWhYrFYqVSy2WypVKKVxcp3amoqnU4zocAPDw/zXfAyz47PIO2FTl+xWKxWq9ls1tzb5P4k8kNjjNFySk9AeKbRzY8sy3RclmXYkHW/+tWvfvvb35pLiLlmsCtLdtVFc1kXXC65YaiX6MTxhZqmxeNx+q+4WjqdpvOraRrlPFX7jDFVVXmhzWQyFHp2woGbLw3LtsDyCjKbnZ2tVCp0yVCX1S9+8Ys//OEPhizypdp3qNPE2t59s2t5Nu1OsUt0ldXr9Uqlwnu1ze2v3UI3zHluqMO9JpvSnEqlZmdnM5mMpmlUJVqm0FwDq6rqphYNIrc5y7rIsho0L3RZ1InYUPJjtFwYoEaj0Wg06vV6oVBoNBq3b9+mJfPz85VKpeHa9PS04c96vT4/P0+bcr8dEd+C4bNhd/V6nSc7l8vx1aanp6vVaqPRqFaruVyOFro5rkqlYrmOuFPD8Xq1trb24Ycf8s88/YTOBbl9+/ba2hp9NuSkmAa7DZq/Pj8/XyqVbt++Xa/XnRNpeRJ9ObPOabY8cQ6JPDHNPjLvzpxUcR1DoaU/6YzwU1OtVkulUuPVklypVMxlrFQq8ZI5Pz9PXzdcGuYEuzyJFE9UKhUqIR4yxTsaS2k0GlQAeLbYXRSGrHMoISeeILvqwizo8u8XyxNHOWaosiqVSqFQ+PDDDylPGo1GtVq9ffu2+MX5+XmxZuBFrr0H7nBp2LUFlleQqFAo0KHlcjk6ZMo0yjd+oflS7TuUWLG2d9/sWp5Ny/S7Z9i7ZfvrslG2Y87zFpPNq0HKjRNTKFYjDS+1qO+53bCpiyyrQfNCT0Wds4xt7AIe330zmF4oFAqFAt0W+DLziR0Hy62MPmiapus63WEXCgW7gUtN02aOXy5quMOgm6dEIrG2ttZ0Miy13qvH02yerCP2jYv/6ty1ablBy69Tr5ubuxnLk9j6mXVOM2vtxPmVPJc8JZVOAb/tprJdKpXoFlMsyWIfaiaTSafTtCbf1NzcHE20KhQK2WzWYafuTyLVOzMzM4F2bRLKN7EQOl8UYtYxL9luWNOhujALtPz7yPLE5fN5OlJDzcl7lRhjiUSiWCzyDk7G2PDwsLg+9T3zL7I2HbjdpdHcaiSdTlM/4traGl8zn8/TaDJdXAatVPt2JbbFaTPi2WQnpd8Ty/bXZaNsxzLPW0k2rwYrlQrvsHdIoaEacV+LEn9z27IusqwGzQs9FXXLyMHHeUcufftkuqqq1JnPh+1al8/nxZFcr6i/On+MzyIyr8abYV+mUCQSCbFdN29TVdUWe57j8Xi1WuV/ivP6DcSrxWE1uw1afp264u3y08DyJLZ4Zon7TPDKl+S1zrkuLpfL1NJns1lqcmgKNf0rn8RNg5vlcplHD4S3/RMTEye2gi5PIl1o7Zr2Hlx5MOzFU3URXPn3kd2Jo3LFW7JkMjk7O5tKpcT5iPQhnU5TcU2n02LtV61WDe1rWw7c8tJoejW+MmOsWCxOTU0Z/omiCsPXW6/2/WU+m5xl+r2ybH9dNsp2HPK8uWTTA3z0Fdq4pxS6r0WDyG3LusiyGjQv9FTU+R0OfdFhoRu6rrs57+bVuu7cuTM7O7u8vDw8PByPx+/fv18ul0Oh0MzMTKFQWFhYUBRlcXHRoQUqFoulUumLL75YWloKhULxeFxRlHv37kmSlEgkJEkql8u3bt1yfzCcLMuLi4s8GV999dWNGzdojhHtjhImy/LOzs7du3cXFhYmJyeXlpZSqZSYBprifePGDVmWJyYmTjwuSZLGxsZ+/etfUwgubnBpaUlRlFqtls1mJUlq4qDMh1YsFrPZ7NjYmOWay8vLpVKJep1VVU2lUux4opUhEyw3aPj6xMREsVi8ceNGOp2enZ3VNG1qasryQCxPol9n1iETxF0Ui0VFUW7evGmZwuAKniVVVe/evUu7o0yzK2OSJN27d09RlIWFBVVVKf2ZTIa+qygKVW137typ1WqKonzyySc///nPJycn6fA5xlgoFEokEnfv3qWiWKvVarXajRs3KEMmJiZu3brlcOW7P4m8eDtf7H7J5XK1Wu3mzZvlcnlsbKxWqy0uLiaTScsybMg6VVUtS4j5orAsS2NjY+bqopWsCzqvnFmeOLGyojyZnJykJalUiq4UWp+ehqRnHVKp1OTkJF2VVGmUy+W5uTnDVdmWA7e8NOgqE8+45WqT9j/UOTw8fOfOnbt377JXM61cLkuSNDMz40u1775Oc9M8iUkVz6au6+b0u08kJWxpaWlhYYGqU8v212Wj7LAjMc+ZTba7TzZjTJKkdDp9//59ylLLFDJTNcIDLJe1qL+5TSxDF8tm0byQ6jeXRZ2vpus6v3ItF7rx+9///uOPP6a61NtqDgPta2trpzOi78xTMur1+omz/dxvsFKp8EkeQfA9JXYbDPpAWtEhxcxf9Xr9xKmxDZtjr9frfHoQZ3kG+ZTr8+Q0y4Ob6uLcs8zwzrwqLS+NplfrTJ2W82eoeIjcp7BDalFDXeQy290XdcsmyWU7ZeZyp4bVzs9vplMXt6ZpXl/iAHDmKIpSqVRUVfX31T+vD1QXAK+5DqlFX5+66PyEmwAAAADQgfAjlgAAAAAQIISbAAAAABAghJsAAAAAECCEmwAAAAAQIISbAACvnUypFProo3anAgBeFwg3AQAAACBACDcBAAAAIEAINwEAAAAgQAg3AQAAACBACDcBAAAAIEAINwEAAAAgQAg3AQAAACBACDcBAAAAIEAINwEAAAAgQAg3AQAAACBACDcBAAAAIEAINwEAAAAgQAg3AQAAACBACDcBAAAAIEAINwEAAAAgQAg3AQAAACBACDcBAAAAIEAINwEAAAAgQKFGo9HuNAAAAADAuYXeTQAAAAAIEMJNAAAAAAgQwk0AAAAACBDCTQAAAAAIUKTdCQAAACNFURhjsiwnEgnnNTVNk2VZlmXxu4VCoVwu+5UYVVV1XU8mkzxhm5ubfX194pJ4PE6Joa/E43FaAgDA0LsJANCBZFmuVCq5XI4HcHYKhYKqquKSZDKp67q/6UmlUjx+LZVKAwMDhiUU72YyGZ4qH+NdADjrEG4CAHScRCKh63o+nxeDtmKxmE6nVVXNZDKzs7O6riuKoqpqqVTKZDI81CO0hK+WTqcZY5qmZTKZYrFot0G7xExPT1cqFerjlGX5gw8+MCyRZTkej8uynEwmk8lkPp8vFAqBZQ8AnDEINwEAOo6maVNTU/F4vF6v84UUEZZKpXw+n8/nKbZLJBIzMzO0RNwCLZybmysWi7y/Mx6P5/N5vk3zBh2SlM1mc7mc8xLGmKIoiqJkMpm5ublWcgAAzhPM3QQA6DiFQkFRlFKppGmaoig0S5JQWOkcGjLGaNJnIpEolUrOa7rcIM3FFAfuzUsYY5VKhTE2MzNz4qxTAHh9INwEAOhEFMbpup7L5cRws0WtTOvM5/Ozs7NiYGq5pKX0AcB51HXnzp12pwEAAL41Ozu7vLw8PDwcj8fv379fLpdDoVAikchkMl988cXS0pKiKBMTExTkTUxMFAqFhYUFRVEWFxcTiYSiKPfu3ZMkib6iquqNGzfGxsbu3bunKMrCwoKqqjdv3pQkyXKDZuIGJUkql8u0NXHJrVu3aLWlpSVKxunmGQB0NPxmOgDA2abruqqqJ/aA0kPueD8RAJw+hJsAAAAAECA8mQ4AAAAAAUK4CQAAAAABQrgJAAAAAAFCuAkAAAAAAUK4CQAAAAABQrgJAAAAAAFCuAkAAAAAAfp/MndWuLQ92VUAAAAASUVORK5CYII=",
      "text/plain": [
       "Tree('S', [('At', 'IN'), ('eight', 'CD'), (\"o'clock\", 'NN'), ('on', 'IN'), ('Thursday', 'NNP'), ('morning', 'NN'), Tree('PERSON', [('Arthur', 'NNP')]), ('did', 'VBD'), (\"n't\", 'RB'), ('feel', 'VB'), ('very', 'RB'), ('good', 'JJ'), ('.', '.')])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities = nltk.chunk.ne_chunk(tagged)\n",
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = treebank.parsed_sents('wsj_0001.mrg')[0]\n",
    "t.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./Data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_question(q):\n",
    "    if q[-1] == '?':\n",
    "        return 1\n",
    "    elif q[-1] == '\"' and q[-2] == '?':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_question(train['text'][6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "for i in range(len(train['text'])):\n",
    "    question = find_question(train['text'][i])\n",
    "    questions.append(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "869"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sylibals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EY1 P R AH0 L',\n",
       " 'IH1 Z',\n",
       " 'DH AH0',\n",
       " 'K R UW1 L AH0 S T',\n",
       " 'M AH1 N TH',\n",
       " 'B R IY1 D IH0 NG',\n",
       " 'L AY1 L AE2 K S',\n",
       " 'AW1 T',\n",
       " 'AH1 V',\n",
       " 'DH AH0',\n",
       " 'D EH1 D']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"april is the cruelest month breeding lilacs out of the dead\"\n",
    "[pronouncing.phones_for_word(p)[0] for p in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_syllable(text):\n",
    "    #remove punctuation and lowercase\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    text = text.split()\n",
    "    phones = []\n",
    "    for i in range(len(text)):\n",
    "        #skips names and non-english words\n",
    "        try:\n",
    "            phone = pronouncing.phones_for_word(text[i])[0]\n",
    "            phones.append(phone)\n",
    "        except IndexError:\n",
    "            pass\n",
    "    try:\n",
    "        d = max([pronouncing.syllable_count(p) for p in phones])\n",
    "    except ValueError:\n",
    "        d = 0\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(text):\n",
    "    #remove punctuation and lowercase\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "    text = text.split() \n",
    "    return len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-8a329e30bd79>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "len(word_count(train['text'][15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_syllable(train['text'][15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "syllables = []\n",
    "for i in range(len(train['text'])):\n",
    "    syllable = find_syllable(train['text'][i])\n",
    "    syllables.append(syllable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19579"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 6,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 6,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 6,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 6,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 5,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 3,\n",
       " 3,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " ...]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Emicant Trabes quos docos vocant.'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['text'][12868]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Past Tense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The surcingle hung in ribands from my body.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.text[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He',\n",
       " 'had',\n",
       " 'escaped',\n",
       " 'me',\n",
       " ',',\n",
       " 'and',\n",
       " 'I',\n",
       " 'must',\n",
       " 'commence',\n",
       " 'a',\n",
       " 'destructive',\n",
       " 'and',\n",
       " 'almost',\n",
       " 'endless',\n",
       " 'journey',\n",
       " 'across',\n",
       " 'the',\n",
       " 'mountainous',\n",
       " 'ices',\n",
       " 'of',\n",
       " 'the',\n",
       " 'ocean',\n",
       " ',',\n",
       " 'amidst',\n",
       " 'cold',\n",
       " 'that',\n",
       " 'few',\n",
       " 'of',\n",
       " 'the',\n",
       " 'inhabitants',\n",
       " 'could',\n",
       " 'long',\n",
       " 'endure',\n",
       " 'and',\n",
       " 'which',\n",
       " 'I',\n",
       " ',',\n",
       " 'the',\n",
       " 'native',\n",
       " 'of',\n",
       " 'a',\n",
       " 'genial',\n",
       " 'and',\n",
       " 'sunny',\n",
       " 'climate',\n",
       " ',',\n",
       " 'could',\n",
       " 'not',\n",
       " 'hope',\n",
       " 'to',\n",
       " 'survive',\n",
       " '.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(train.text[15])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.ConditionalFreqDist(train.text[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('He', 'PRP'),\n",
       " ('had', 'VBD'),\n",
       " ('escaped', 'VBN'),\n",
       " ('me', 'PRP'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('I', 'PRP'),\n",
       " ('must', 'MD'),\n",
       " ('commence', 'VB'),\n",
       " ('a', 'DT'),\n",
       " ('destructive', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('almost', 'RB'),\n",
       " ('endless', 'JJ'),\n",
       " ('journey', 'NN'),\n",
       " ('across', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('mountainous', 'JJ'),\n",
       " ('ices', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('ocean', 'NN'),\n",
       " (',', ','),\n",
       " ('amidst', 'NN'),\n",
       " ('cold', 'NN'),\n",
       " ('that', 'IN'),\n",
       " ('few', 'JJ'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('inhabitants', 'NNS'),\n",
       " ('could', 'MD'),\n",
       " ('long', 'VB'),\n",
       " ('endure', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('which', 'WDT'),\n",
       " ('I', 'PRP'),\n",
       " (',', ','),\n",
       " ('the', 'DT'),\n",
       " ('native', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('genial', 'JJ'),\n",
       " ('and', 'CC'),\n",
       " ('sunny', 'JJ'),\n",
       " ('climate', 'NN'),\n",
       " (',', ','),\n",
       " ('could', 'MD'),\n",
       " ('not', 'RB'),\n",
       " ('hope', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('survive', 'VB'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(tokens)\n",
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConditionalFreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def past_tense(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    pasts = 0\n",
    "    new_tagged = []\n",
    "    while \n",
    "    for i in range(len(tagged)):\n",
    "        if tagged[i][1] == 'VBD' or tagged[i][1]== 'VBN':\n",
    "            pasts += 1\n",
    "    return pasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_tense(train.text[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def present_tense(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    presents = 0\n",
    "    for i in range(len(tagged)):\n",
    "        if tagged[i][1] == 'VBP' or tagged[i][1]== 'VBG':\n",
    "            presents += 1\n",
    "    return presents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "present_tense(train.text[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_tense_input(sentance):\n",
    "    text = nltk.word_tokenize(sentance)\n",
    "    tagged = nltk.pos_tag(text)\n",
    "\n",
    "    tense = {}\n",
    "    tense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n",
    "    tense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n",
    "    tense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]]) \n",
    "    return(tense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HPL'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.author[180]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = determine_tense_input(train.text[180])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['future']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  He/PRP\n",
      "  had/VBD\n",
      "  escaped/VBN\n",
      "  me/PRP\n",
      "  ,/,\n",
      "  and/CC\n",
      "  I/PRP\n",
      "  must/MD\n",
      "  commence/VB\n",
      "  (NP a/DT destructive/NN)\n",
      "  and/CC\n",
      "  almost/RB\n",
      "  (NP endless/JJ journey/NN)\n",
      "  across/IN\n",
      "  the/DT\n",
      "  mountainous/JJ\n",
      "  ices/NNS\n",
      "  of/IN\n",
      "  (NP the/DT ocean/NN)\n",
      "  ,/,\n",
      "  (NP amidst/NN)\n",
      "  (NP cold/NN)\n",
      "  that/IN\n",
      "  few/JJ\n",
      "  of/IN\n",
      "  the/DT\n",
      "  inhabitants/NNS\n",
      "  could/MD\n",
      "  long/VB\n",
      "  (NP endure/NN)\n",
      "  and/CC\n",
      "  which/WDT\n",
      "  I/PRP\n",
      "  ,/,\n",
      "  (NP the/DT native/NN)\n",
      "  of/IN\n",
      "  a/DT\n",
      "  genial/JJ\n",
      "  and/CC\n",
      "  (NP sunny/JJ climate/NN)\n",
      "  ,/,\n",
      "  could/MD\n",
      "  not/RB\n",
      "  hope/VB\n",
      "  to/TO\n",
      "  survive/VB\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(tagged)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"),(\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = cp.parse(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  (NP the/DT cat/NN))\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "  NP:\n",
    "    {<.*>+}          # Chunk everything\n",
    "    }<VBD|IN>+{      # Chink sequences of VBD and IN\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"),\n",
    "       (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DT', 'B-NP'), ('little', 'JJ', 'I-NP'), ('yellow', 'JJ', 'I-NP'), ('dog', 'NN', 'I-NP'), ('barked', 'VBD', 'O'), ('at', 'IN', 'O'), ('the', 'DT', 'B-NP'), ('cat', 'NN', 'I-NP')]\n"
     ]
    }
   ],
   "source": [
    "iob_tags = tree2conlltags(cp.parse(sentence))\n",
    "print(iob_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breaking down scentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk import conlltags2tree, tree2conlltags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In',\n",
       " 'his',\n",
       " 'left',\n",
       " 'hand',\n",
       " 'was',\n",
       " 'a',\n",
       " 'gold',\n",
       " 'snuff',\n",
       " 'box',\n",
       " ',',\n",
       " 'from',\n",
       " 'which',\n",
       " ',',\n",
       " 'as',\n",
       " 'he',\n",
       " 'capered',\n",
       " 'down',\n",
       " 'the',\n",
       " 'hill',\n",
       " ',',\n",
       " 'cutting',\n",
       " 'all',\n",
       " 'manner',\n",
       " 'of',\n",
       " 'fantastic',\n",
       " 'steps',\n",
       " ',',\n",
       " 'he',\n",
       " 'took',\n",
       " 'snuff',\n",
       " 'incessantly',\n",
       " 'with',\n",
       " 'an',\n",
       " 'air',\n",
       " 'of',\n",
       " 'the',\n",
       " 'greatest',\n",
       " 'possible',\n",
       " 'self',\n",
       " 'satisfaction',\n",
       " '.']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(train.text[2])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('In', 'IN'),\n",
       " ('his', 'PRP$'),\n",
       " ('left', 'JJ'),\n",
       " ('hand', 'NN'),\n",
       " ('was', 'VBD'),\n",
       " ('a', 'DT'),\n",
       " ('gold', 'JJ'),\n",
       " ('snuff', 'NN'),\n",
       " ('box', 'NN'),\n",
       " (',', ','),\n",
       " ('from', 'IN'),\n",
       " ('which', 'WDT'),\n",
       " (',', ','),\n",
       " ('as', 'IN'),\n",
       " ('he', 'PRP'),\n",
       " ('capered', 'VBD'),\n",
       " ('down', 'RP'),\n",
       " ('the', 'DT'),\n",
       " ('hill', 'NN'),\n",
       " (',', ','),\n",
       " ('cutting', 'VBG'),\n",
       " ('all', 'DT'),\n",
       " ('manner', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('fantastic', 'JJ'),\n",
       " ('steps', 'NNS'),\n",
       " (',', ','),\n",
       " ('he', 'PRP'),\n",
       " ('took', 'VBD'),\n",
       " ('snuff', 'NN'),\n",
       " ('incessantly', 'RB'),\n",
       " ('with', 'IN'),\n",
       " ('an', 'DT'),\n",
       " ('air', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('greatest', 'JJS'),\n",
       " ('possible', 'JJ'),\n",
       " ('self', 'NN'),\n",
       " ('satisfaction', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(tokens)\n",
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "  NP:\n",
    "    {<.*>+}          # Chunk everything\n",
    "    }<VBD|IN>+{      # Chink sequences of VBD and IN\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('In', 'IN', 'O'), ('his', 'PRP$', 'B-NP'), ('left', 'JJ', 'I-NP'), ('hand', 'NN', 'I-NP'), ('was', 'VBD', 'O'), ('a', 'DT', 'B-NP'), ('gold', 'JJ', 'I-NP'), ('snuff', 'NN', 'I-NP'), ('box', 'NN', 'I-NP'), (',', ',', 'I-NP'), ('from', 'IN', 'O'), ('which', 'WDT', 'B-NP'), (',', ',', 'I-NP'), ('as', 'IN', 'O'), ('he', 'PRP', 'B-NP'), ('capered', 'VBD', 'O'), ('down', 'RP', 'B-NP'), ('the', 'DT', 'I-NP'), ('hill', 'NN', 'I-NP'), (',', ',', 'I-NP'), ('cutting', 'VBG', 'I-NP'), ('all', 'DT', 'I-NP'), ('manner', 'NN', 'I-NP'), ('of', 'IN', 'O'), ('fantastic', 'JJ', 'B-NP'), ('steps', 'NNS', 'I-NP'), (',', ',', 'I-NP'), ('he', 'PRP', 'I-NP'), ('took', 'VBD', 'O'), ('snuff', 'NN', 'B-NP'), ('incessantly', 'RB', 'I-NP'), ('with', 'IN', 'O'), ('an', 'DT', 'B-NP'), ('air', 'NN', 'I-NP'), ('of', 'IN', 'O'), ('the', 'DT', 'B-NP'), ('greatest', 'JJS', 'I-NP'), ('possible', 'JJ', 'I-NP'), ('self', 'NN', 'I-NP'), ('satisfaction', 'NN', 'I-NP'), ('.', '.', 'I-NP')]\n"
     ]
    }
   ],
   "source": [
    "iob_tags = tree2conlltags(cp.parse(tagged))\n",
    "print(iob_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP He/PRP)\n",
      "  had/VBD\n",
      "  (NP\n",
      "    escaped/VBN\n",
      "    me/PRP\n",
      "    ,/,\n",
      "    and/CC\n",
      "    I/PRP\n",
      "    must/MD\n",
      "    commence/VB\n",
      "    a/DT\n",
      "    destructive/NN\n",
      "    and/CC\n",
      "    almost/RB\n",
      "    endless/JJ\n",
      "    journey/NN)\n",
      "  across/IN\n",
      "  (NP the/DT mountainous/JJ ices/NNS)\n",
      "  of/IN\n",
      "  (NP the/DT ocean/NN ,/, amidst/NN cold/NN)\n",
      "  that/IN\n",
      "  (NP few/JJ)\n",
      "  of/IN\n",
      "  (NP\n",
      "    the/DT\n",
      "    inhabitants/NNS\n",
      "    could/MD\n",
      "    long/VB\n",
      "    endure/NN\n",
      "    and/CC\n",
      "    which/WDT\n",
      "    I/PRP\n",
      "    ,/,\n",
      "    the/DT\n",
      "    native/NN)\n",
      "  of/IN\n",
      "  (NP\n",
      "    a/DT\n",
      "    genial/JJ\n",
      "    and/CC\n",
      "    sunny/JJ\n",
      "    climate/NN\n",
      "    ,/,\n",
      "    could/MD\n",
      "    not/RB\n",
      "    hope/VB\n",
      "    to/TO\n",
      "    survive/VB\n",
      "    ./.))\n"
     ]
    }
   ],
   "source": [
    "tree = conlltags2tree(iob_tags)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B-NP'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iob_tags[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "  NP:\n",
    "    {<.*>+}          # Chunk everything\n",
    "    }<VBD|IN>+{      # Chink sequences of VBD and IN\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_tense(sentance):\n",
    "    text = nltk.word_tokenize(sentance)\n",
    "    tagged = nltk.pos_tag(text)\n",
    "    iob_tags = tree2conlltags(cp.parse(tagged))\n",
    "\n",
    "    tense = {}\n",
    "    tense[\"future\"] = len([word for word in iob_tags if word[1] == \"MD\"] and word[2] == 'O')\n",
    "    tense[\"present\"] = len([word for word in iob_tags if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]] and word[2] == 'O')\n",
    "    tense[\"past\"] = len([word for word in iob_tags if word[1] in [\"VBD\", \"VBN\"]] and word[2] == 'O') \n",
    "    return(tense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-4696a4535b7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdetermine_tense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-87-04590d90ce56>\u001b[0m in \u001b[0;36mdetermine_tense\u001b[1;34m(sentance)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtense\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtense\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"future\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miob_tags\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"MD\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'O'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mtense\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"present\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miob_tags\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"VBP\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"VBZ\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"VBG\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'O'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mtense\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"past\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miob_tags\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"VBD\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"VBN\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'O'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word' is not defined"
     ]
    }
   ],
   "source": [
    "determine_tense(train.text[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_tense_input(sentance):\n",
    "    text = nltk.word_tokenize(sentance)\n",
    "    tagged = nltk.pos_tag(text)\n",
    "\n",
    "    tense = {}\n",
    "    tense[\"future\"] = len([word for word in tagged if word[1] == \"MD\"])\n",
    "    tense[\"present\"] = len([word for word in tagged if word[1] in [\"VBP\", \"VBZ\",\"VBG\"]])\n",
    "    tense[\"past\"] = len([word for word in tagged if word[1] in [\"VBD\", \"VBN\"]]) \n",
    "    return(tense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def past_tense(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    iob_tags = tree2conlltags(cp.parse(tagged))\n",
    "    pasts = 0\n",
    "    new_tagged = []\n",
    "    for i in range(len(iob_tags)):\n",
    "        if iob_tags[i][1] in [\"VBD\", \"VBN\"] and iob_tags[i][2] == 'O':\n",
    "            pasts += 1\n",
    "    return pasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "past_tense(train.text[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def present_tense(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    iob_tags = tree2conlltags(cp.parse(tagged))\n",
    "    presents = 0\n",
    "    for i in range(len(tagged)):\n",
    "        if iob_tags[i][1] in [\"VBP\", \"VBZ\",\"VBG\"] and iob_tags[i][2] == 'O':\n",
    "            presents += 1\n",
    "    return presents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "present_tense(train.text[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'future': 0, 'present': 1, 'past': 3}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "determine_tense_input(train.text[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Textblob"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
